import os
import re
import json
from typing import Dict, List, Optional


class VietnameseDangerousContentDetector:
    """
    A rule-based system for detecting dangerous content in Vietnamese text.
    Supports multiple categories of dangerous content with configurable severity levels.
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """
        Initialize the detector with default or custom configuration.
        
        Args:
            config_path: Optional path to a JSON configuration file
        """
        # Default keyword categories with varying severity levels
        self.default_keywords = {
            "violence": {
                "keywords": ["gi·∫øt ng∆∞·ªùi", "ƒë√°nh nhau", "b·∫°o l·ª±c", "t·∫•n c√¥ng", "ƒë√¢m", "ch√©m", 
                            "h√†nh hung", "ƒëe d·ªça", "tr·∫£ th√π", "h√†nh quy·∫øt", "s√°t h·∫°i"],
                "severity": 3
            },
            "terrorism": {
                "keywords": ["kh·ªßng b·ªë", "bom", "ƒë√°nh bom", "t·ª± s√°t", "ph√° ho·∫°i", 
                            "c·ª±c ƒëoan", "b·∫°o ƒë·ªông", "kh·ªßng b·ªë", "k√≠ch ƒë·ªông"],
                "severity": 4
            },
            "weapons": {
                "keywords": ["s√∫ng", "ƒë·∫°n", "v≈© kh√≠", "dao", "thu·ªëc n·ªï", "m√¨n", 
                            "l·ª±u ƒë·∫°n", "v≈© trang", "ch·∫•t n·ªï"],
                "severity": 2
            },
            "drugs": {
                "keywords": ["ma t√∫y", "heroin", "c·∫ßn sa", "cocaine", "thu·ªëc l·∫Øc", 
                            "ch·∫•t g√¢y nghi·ªán", "ti√™m ch√≠ch", "ch·∫•t k√≠ch th√≠ch"],
                "severity": 2
            },
            "political_extremism": {
                "keywords": ["l·∫≠t ƒë·ªï", "ph·∫£n ƒë·ªông", "ch·ªëng ph√°", "ph√° ho·∫°i", 
                            "ch·ªëng ƒë·ªëi", "√¢m m∆∞u", "g√¢y r·ªëi", "b·∫°o lo·∫°n"],
                "severity": 3
            },
            "hate_speech": {
                "keywords": ["n√¥ng c·∫°n", "b·ª©c x√∫c", "th√π h·∫±n", "k·ª≥ th·ªã", "ph√¢n bi·ªát", 
                            "gh√©t b·ªè", "x√∫c ph·∫°m", "nh·ª•c m·∫°", "nh·∫°o b√°ng"],
                "severity": 1
            }
        }
        
        # Load custom configuration if provided
        self.keywords = self.default_keywords
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    custom_config = json.load(f)
                    self.keywords.update(custom_config)
            except Exception as e:
                print(f"L·ªói khi t·∫£i t·ªáp c·∫•u h√¨nh: {e}")
                print("S·ª≠ d·ª•ng c·∫•u h√¨nh m·∫∑c ƒë·ªãnh")
        
        # Compile regex patterns for each category for faster matching
        self.patterns = {}
        for category, data in self.keywords.items():
            pattern = r'\b(' + '|'.join(re.escape(kw) for kw in data["keywords"]) + r')\b'
            self.patterns[category] = re.compile(pattern, re.IGNORECASE)
    
    def analyze_text(self, text: str, min_severity: int = 1) -> Dict:
        """
        Analyze text and identify dangerous content above the minimum severity threshold.
        
        Args:
            text: The Vietnamese text to analyze
            min_severity: Minimum severity level to report (1-4)
            
        Returns:
            Dictionary with analysis results and matched content
        """
        results = {
            "is_dangerous": False,
            "matches": {},
            "highest_severity": 0,
            "dangerous_categories": []
        }
        
        # Check each category
        for category, pattern in self.patterns.items():
            severity = self.keywords[category]["severity"]
            if severity < min_severity:
                continue
                
            matches = pattern.findall(text)
            if matches:
                unique_matches = list(set(matches))
                results["matches"][category] = {
                    "keywords": unique_matches,
                    "severity": severity,
                    "count": len(matches)
                }
                results["is_dangerous"] = True
                results["dangerous_categories"].append(category)
                results["highest_severity"] = max(results["highest_severity"], severity)
        
        return results
    
    def analyze_title(self, title: str, min_severity: int = 1) -> Dict:
        """
        Analyze video title for dangerous content.
        
        Args:
            title: The video title to analyze
            min_severity: Minimum severity level to report
            
        Returns:
            Dictionary with analysis results
        """
        # Use the same analysis method but mark it specifically for title
        results = self.analyze_text(title, min_severity)
        results["content_type"] = "title"
        return results
    
    def analyze_comments(self, comments: List[Dict], min_severity: int = 1) -> Dict:
        """
        Analyze a list of comments for dangerous content.
        
        Args:
            comments: List of comment dictionaries with 'text' key
            min_severity: Minimum severity level to report
            
        Returns:
            Dictionary with analysis results and dangerous comments
        """
        combined_results = {
            "is_dangerous": False,
            "highest_severity": 0,
            "dangerous_categories": set(),
            "content_type": "comments",
            "total_comments": len(comments),
            "dangerous_comments": [],
            "dangerous_comment_count": 0
        }
        
        for i, comment in enumerate(comments):
            comment_text = comment.get('text', '') or comment.get('comment_text', '')
            if not comment_text:
                continue
                
            # Analyze individual comment
            result = self.analyze_text(comment_text, min_severity)
            
            if result["is_dangerous"]:
                # Add this comment to the dangerous comments list
                dangerous_comment = {
                    "index": i,
                    "comment_data": comment,
                    "analysis": result
                }
                combined_results["dangerous_comments"].append(dangerous_comment)
                combined_results["dangerous_comment_count"] += 1
                combined_results["is_dangerous"] = True
                combined_results["highest_severity"] = max(
                    combined_results["highest_severity"], 
                    result["highest_severity"]
                )
                
                # Add all dangerous categories found
                for category in result["dangerous_categories"]:
                    combined_results["dangerous_categories"].add(category)
        
        # Convert set to list for JSON serialization
        combined_results["dangerous_categories"] = list(combined_results["dangerous_categories"])
        
        return combined_results
    
    def format_report(self, results: Dict, verbose: bool = False) -> str:
        """
        Format analysis results into a human-readable report.
        
        Args:
            results: Analysis results from analyze_text
            verbose: Whether to include detailed match information
            
        Returns:
            Formatted report as a string
        """
        content_type = results.get("content_type", "content")
        
        if results.get("is_dangerous", False):
            report = f"üî¥ Ph√°t hi·ªán n·ªôi dung nguy hi·ªÉm trong {content_type}!\n"
            report += f"M·ª©c ƒë·ªô nghi√™m tr·ªçng: {results['highest_severity']}/4\n\n"
            
            if verbose:
                report += "Chi ti·∫øt:\n"
                
                if content_type == "comments":
                    report += f"Ph√°t hi·ªán {results['dangerous_comment_count']} b√¨nh lu·∫≠n nguy hi·ªÉm " \
                             f"trong t·ªïng s·ªë {results['total_comments']} b√¨nh lu·∫≠n.\n\n"
                    
                    for i, comment in enumerate(results["dangerous_comments"][:5]):  # Limit to first 5
                        report += f"B√¨nh lu·∫≠n #{i+1}:\n"
                        report += f"Ng∆∞·ªùi d√πng: {comment['comment_data'].get('author', 'Unknown')}\n"
                        report += f"N·ªôi dung: {comment['comment_data'].get('text', '')[:100]}...\n"
                        report += f"M·ª©c ƒë·ªô nghi√™m tr·ªçng: {comment['analysis']['highest_severity']}/4\n"
                        report += f"Danh m·ª•c: {', '.join(comment['analysis']['dangerous_categories'])}\n\n"
                    
                    if len(results["dangerous_comments"]) > 5:
                        report += f"... v√† {len(results['dangerous_comments']) - 5} b√¨nh lu·∫≠n nguy hi·ªÉm kh√°c.\n"
                
                else:
                    # For title or transcription
                    for category in results["dangerous_categories"]:
                        if "matches" in results and category in results["matches"]:
                            matches = results["matches"][category]
                            report += f"- {category.upper()} (M·ª©c ƒë·ªô: {matches['severity']}/4)\n"
                            report += f"  T·ª´ kh√≥a: {', '.join(matches['keywords'])}\n"
                            report += f"  S·ªë l·∫ßn xu·∫•t hi·ªán: {matches['count']}\n"
        else:
            report = f"‚úÖ Kh√¥ng ph√°t hi·ªán n·ªôi dung nguy hi·ªÉm trong {content_type}"
        
        return report
    
    def add_custom_category(self, category_name: str, keywords: List[str], severity: int = 2) -> None:
        """
        Add a custom category of dangerous content with keywords and severity.
        
        Args:
            category_name: Name of the new category
            keywords: List of keywords for the category
            severity: Severity level (1-4)
        """
        if severity < 1 or severity > 4:
            raise ValueError("M·ª©c ƒë·ªô nghi√™m tr·ªçng ph·∫£i t·ª´ 1 ƒë·∫øn 4")
            
        self.keywords[category_name] = {
            "keywords": keywords,
            "severity": severity
        }
        
        # Update the regex pattern for the new category
        pattern = r'\b(' + '|'.join(re.escape(kw) for kw in keywords) + r')\b'
        self.patterns[category_name] = re.compile(pattern, re.IGNORECASE)
    
    def save_config(self, config_path: str) -> None:
        """
        Save the current keyword configuration to a file.
        
        Args:
            config_path: Path to save the configuration JSON file
        """
        try:
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(self.keywords, f, ensure_ascii=False, indent=2)
            print(f"ƒê√£ l∆∞u c·∫•u h√¨nh th√†nh c√¥ng v√†o {config_path}")
        except Exception as e:
            print(f"L·ªói khi l∆∞u c·∫•u h√¨nh: {e}")


# Example usage
if __name__ == "__main__":
    # Create a detector with default configuration
    detector = VietnameseDangerousContentDetector()
    
    # Add a custom category if needed
    detector.add_custom_category(
        "financial_crime", 
        ["r·ª≠a ti·ªÅn", "l·ª´a ƒë·∫£o", "chi·∫øm ƒëo·∫°t", "bi·ªÉn th·ªß", "tr·ªën thu·∫ø"],
        severity=2
    )
    
    # Example text analysis
    text = """
    C√°c ƒë·ªëi t∆∞·ª£ng ƒë√£ l√™n k·∫ø ho·∫°ch t·∫•n c√¥ng v√†o t√≤a nh√† ch√≠nh ph·ªß v√† s·ª≠ d·ª•ng
    bom t·ª± ch·∫ø ƒë·ªÉ g√¢y ra ho·∫£ng lo·∫°n. Ch√∫ng c√≤n bu√¥n b√°n ma t√∫y ƒë·ªÉ g√¢y qu·ªπ cho
    ho·∫°t ƒë·ªông kh·ªßng b·ªë.
    """
    
    # Analyze the text
    results = detector.analyze_text(text)
    
    # Print the formatted report
    print(detector.format_report(results, verbose=True))
    
    # Example title analysis
    title = "C√°c ƒë·ªëi t∆∞·ª£ng t·∫•n c√¥ng v√† ƒëe d·ªça c·∫£nh s√°t"
    title_results = detector.analyze_title(title)
    print("\nTitle Analysis:")
    print(detector.format_report(title_results, verbose=True))
    
    # Example comments analysis
    comments = [
        {"author": "User1", "text": "B√†i vi·∫øt hay v√† b·ªï √≠ch"},
        {"author": "User2", "text": "T√¥i s·∫Ω t·∫•n c√¥ng b·∫•t k·ª≥ ai ch·ªëng ƒë·ªëi t√¥i v·ªõi s√∫ng c·ªßa t√¥i"},
        {"author": "User3", "text": "Qu√° nhi·ªÅu b·∫°o l·ª±c trong x√£ h·ªôi ng√†y nay"}
    ]
    comment_results = detector.analyze_comments(comments)
    print("\nComments Analysis:")
    print(detector.format_report(comment_results, verbose=True))


# Simple function example without using the class
def check_dangerous_content(text, min_severity=1):
    """
    Simple function to check for dangerous content in Vietnamese text.
    
    Args:
        text: Text to analyze
        min_severity: Minimum severity level (1-4)
        
    Returns:
        Tuple of (is_dangerous, report_string)
    """
    detector = VietnameseDangerousContentDetector()
    results = detector.analyze_text(text, min_severity)
    report = detector.format_report(results, verbose=True)
    return results["is_dangerous"], report